{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMqbXfOUblazZc3onHis2mx"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **HugginFace - Transformers - Training a Model** ###\n",
        "\n",
        "Anteriormente hemos utilizado un modelo preentrenado para hacer una clasificación de nuestros textos de entrada como Positivo o Negativo. En este caso, vamos a entrenar un modelo para poder clasificar nuestros textos por número de Estrellas. Para ellos, tomaremos un modelo y lo entrenaremos con un [Dataset](https://huggingface.co/datasets) escogido a tal efecto.\n",
        "\n",
        "## *Modulos:* ##"
      ],
      "metadata": {
        "id": "GZnrnNH1N79K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Antes de ejecutar la instrucción de instalación de datasets, es neceario desinstalar pyarrow (la versión preinstalada en Colab no es complatible)\n",
        "# Tras desistalar, es necesario Reiniciar la sesión en el entorno de ejecución\n",
        "!pip uninstall -y pyarrow"
      ],
      "metadata": {
        "id": "jVuSDUHoSIqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tras la instalación, es necesario Reiniciar la sesión en el entorno de ejecución\n",
        "!pip install transformers evaluate datasets accelerate -q"
      ],
      "metadata": {
        "id": "o8TTn0t4QR52"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "a7MI3GekPwY1"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Dataset:* ##\n",
        "\n",
        "Vamos a trabajar con el Dataset \"yelp_review_full\", que contiene comentarios sobre películas y su clasificación en 1-5 estrellas."
      ],
      "metadata": {
        "id": "zf3iBVbUhJQR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhqQPZyrNLp6"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"yelp_review_full\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'][0] # Visualizamos el primer comentario del Conjunto de Datos, clasificado con 4 estrellas."
      ],
      "metadata": {
        "id": "5MreRsGOVAK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dado que el Dataset con el que vamos a trabajar es muy grande, existe la posibilidad de comenzar a trabajar con un subconjunto de los datos para hacer pruebas y posteriormente entrenar el modelo con el conjunto de datos completo."
      ],
      "metadata": {
        "id": "pMi_QqTPiOnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "small_train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(80))\n",
        "small_eval_dataset = dataset[\"test\"].shuffle(seed=42).select(range(40))"
      ],
      "metadata": {
        "id": "Zz_3u1dqVKvq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Tokenizar el Dataset:* ##\n",
        "\n",
        "Antes de pasarle los datos de entrada al modelo, es necesario Tokenizar los textos de entrada. Para ello, vamos a utilizar el modelo \"bert-base-cased\"."
      ],
      "metadata": {
        "id": "m913dFh-jPaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "LPPUIRumkVME"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo = \"bert-base-cased\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(modelo)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "small_train_dataset = small_train_dataset.map(tokenize_function, batched=True)\n",
        "small_eval_dataset = small_eval_dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "knVarPnGkbAB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Modelo:* ##\n",
        "\n",
        "Creamos el modelo \"bert-base-cased\"."
      ],
      "metadata": {
        "id": "bmhijQdWlCzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification"
      ],
      "metadata": {
        "id": "0c6BPdCClJHp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(modelo, num_labels=5)"
      ],
      "metadata": {
        "id": "yYDSDokmlPvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Entrenamiento:* ##\n",
        "\n",
        "Vamos a entrenar el modelo haciendo uso de los datos disponibles en el Dataset."
      ],
      "metadata": {
        "id": "29vKsFF9ltQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "from transformers import TrainingArguments, Trainer"
      ],
      "metadata": {
        "id": "zWLoqIxrl1uZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = evaluate.load(\"accuracy\")"
      ],
      "metadata": {
        "id": "7lWKHgXAmWub"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos la función compute_metrics que nos devolverá la exactitud de las predicciones\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "7r0kDqUkmdAq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos los [argumentos](https://huggingface.co/transformers/v4.2.2/main_classes/trainer.html) del entrenamiento"
      ],
      "metadata": {
        "id": "gRuhkR3Tnn8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    'clasificador_estrellas',\n",
        "    evaluation_strategy=\"steps\",\n",
        "    logging_steps=5,\n",
        "    num_train_epochs=1,\n",
        "    push_to_hub=True,\n",
        ")"
      ],
      "metadata": {
        "id": "NMCIOz8LnKI7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instanciamos el objeto Trainer"
      ],
      "metadata": {
        "id": "i6sOwzZko7PE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    train_dataset = small_train_dataset,\n",
        "    eval_dataset = small_eval_dataset,\n",
        "    compute_metrics = compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "ACa-H22lo-9G"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalmente, entrenamos el modelo."
      ],
      "metadata": {
        "id": "nvzTvKO8x0bY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamiento\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "j2EC3Qxbou5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Guardar el modelo en local:* ##"
      ],
      "metadata": {
        "id": "FQM3AMkbx4i_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"./modelo\")"
      ],
      "metadata": {
        "id": "HOn9MyTzyGmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Subir el modelo a Hugging Face:* ##"
      ],
      "metadata": {
        "id": "EXKZBfw-yact"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub()\n",
        "tokenizer.push_to_hub('Pegaso183/clasificador_estrellas')"
      ],
      "metadata": {
        "id": "SZkdJ5hJyiDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PENDIENTE TRABAJAR EN LA MEJORA DEL MODELO\n",
        "\n",
        "* Mejorar la eficiencia del entrenamiento mediante Lora.\n",
        "* Incrementar los datos del Dataset de entrada utilizados.\n",
        "* Incrementar el número de épocas de entrenamiento.\n",
        "* Mejorar la parametrización del entrenamiento."
      ],
      "metadata": {
        "id": "oMBdou2r48qe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_GoP_5rN5W5V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}