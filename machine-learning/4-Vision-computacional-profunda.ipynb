{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59724343-33e6-4fe4-adf1-0e4a6172d683",
   "metadata": {},
   "source": [
    "### **Visión Computacional Profunda** ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a3e7b7-e220-4819-b7d2-cc3eb6997851",
   "metadata": {},
   "source": [
    "Vamos a trabajar con una Red Neuronal Convolucional (Convolutional Neural Network) para clasificación de imágenes y detección/reconocimiento de objetos, usando Visión Computacional Profunda (Deep Computer Vision).\n",
    "\n",
    "El objetivo es clasificar y detectar objetos específicos en una imagen dada. A continuación se detallan algunos de los conceptos inherentes a las Redes Convolucionales:\n",
    "\n",
    "* Datos de Imagen (Image Data)\n",
    "* Capa Convolucional (Convolutional Layer): Se aplican operaciones de convolución para detectar patrones locales en la entrada, como bordes, texturas y formas.\n",
    "* Capa de Agrupación (Pooling Layer): Su objetivo es reducir la dimensionalidad de la representación, conservando las características más importantes y disminuyendo el número de parámetros.\n",
    "* Arquitecturas CNN (Convolutional Neural Network Architectures)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "18862747-e378-4915-8ca9-1c9027012f4b",
   "metadata": {},
   "source": [
    "**Datos de Imagen:**\n",
    "\n",
    "Las imágenes con las que trataremos en este caso tienen 3 dimensiones (Mapa de Características): Alto, Ancho y Canales de Color. El número e canales de color representa la profundidad de una imagen y los colores utilizados en ella. Por ejemplo, una imagen con tres canales estará compuesta comunmente por píxeles RGB (red, green, blue). Por tanto, por cada pixel tenemos tres valores numéricos en el rango 0-255. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bc36a120-721b-4865-aa66-7291516dbb19",
   "metadata": {},
   "source": [
    "**Redes Neuronales Convolucionales (Convnet):**\n",
    "\n",
    "Una Convnet está compuesta por N Capas Convolucionales. Su objetivo es detectar patrones en la imagen que puedan ser utilizados para clasificar la imágen o partes de ella.\n",
    "La diferencia fundamental entre estás capas y las capas densamente conectadas es que estás últimas detentan patrones globalmente, mientras que las capas convolucionales detectan patrones localmente (sin tener en consideración en qué punto de la imagen detecta dichos patrones).\n",
    "\n",
    "Las capas convolucionales tomarán mapas de características como entrada y nos darán como resultado otro mapa de características detectadas en la imagen, a las cuáles nos referiremos como Filtros."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8233cd7b-5430-41b3-bdaf-dbc0fc435f35",
   "metadata": {},
   "source": [
    "**Parámetros de Capa:**\n",
    "\n",
    "Una Capa Convolucional está definida por dos parámetros clave: Filtros y \n",
    "\n",
    "* Filtros: Patrón de m x n pixels que se busca en una imagen. El número de filtros en una Capa Convolucional represente la catidad de patrones que la capa está buscando y la profundidad del Mapa de Características resultante.\n",
    "* "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f56a676b-fd2e-40bc-83c0-e1c6d112020f",
   "metadata": {},
   "source": [
    "**[Creación de una Convnet](https://www.tensorflow.org/tutorials/images/cnn):**\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2c69ae02-a7e9-4acc-ad60-09123a08c394",
   "metadata": {},
   "source": [
    "**Dataset:**\n",
    "\n",
    "En el ejemplo vamos a tratar de clasificar 10 diferentes objetos. El Dataset que vamos a utilizar está dentro de Tensorflow y se llama \"CIFAR Image Dataset\". Contiene 60.000 imágenes de color 32x32 con 6.000 imágenes pertenecientes a cada clasificación. Las etiquetas son: Airplane, Automobile, Bird, Cat, Deer, Dog, Frog, Horse, Ship, Truck."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ba82e5b5-7540-4323-b473-be4149038817",
   "metadata": {},
   "source": [
    "**Modulos:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea97c0d6-3159-42c5-9841-e6b4f1122b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2727ff61-6cc4-4e5d-9a55-68fccf34f978",
   "metadata": {},
   "source": [
    "**Carga de los datos de entrada:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db81239-bf13-440a-b4e9-39abfe0176ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos y partimos (datos de entrenamiento y datos de evaluación) el Dataset \n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
    "\n",
    "# Normalizamos los valores de cada pixel para que estén en el ranto 0-1\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f01946b-0e7e-40a4-9d38-de7188732864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de imagen\n",
    "img_index = 0\n",
    "plt.imshow(train_images[img_index], cmap=plt.cm.binary)\n",
    "plt.xlabel(class_names[train_labels[img_index][0]]) # Es necesario un índice adicional [0] porque las etiquetas del Dataset son Arrays\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf54c40-9da0-4f55-bb7b-7d6fa159b963",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i])\n",
    "    # The CIFAR labels happen to be arrays, \n",
    "    # which is why you need the extra index\n",
    "    plt.xlabel(class_names[train_labels[i][0]])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e942628d-6dbb-4b7d-a5eb-3e7ed31f0e62",
   "metadata": {},
   "source": [
    "**Arquitectura de la Red Neuronal Convolucional:**\n",
    "\n",
    "Una arquitectura común para las CNNs es una pila de capas Conv2D y MaxPooling2D, seguidas por unas pocas capas densamente conectadas. La idea es que las capas convolucionales y de maxPooling (su objetivo es simplificar los Mapas de Características resultantes) extraen las características importantes de la imagen (Filtros). Tras lo cual las capas densamente conectadas determinan la clase a la que pertenece la imagen. Comenzamos con la construcción de la Base Convolucional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500d6aa0-6143-42d3-a014-0d49bf42f26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e127cf-91c8-4e6f-9f4f-b6ee4f2742a4",
   "metadata": {},
   "source": [
    "Como entrada, una CNN toma tensores de forma (altura_imagen, ancho_imagen, canales_color), ignorando el tamaño del lote.\n",
    "\n",
    "Hemos creado 3 Capas Convolucionales y 2 Capas de MaxPooling entre ellas:\n",
    "* Capa 1: Los datos de entrada son de (32,32,3) y procesan 32 filtros de (3,3). Se aplica la función de activación relu.\n",
    "* Capa 2: Ejecuta la operación de max pooling utilizando ejemplos de 2x2 y un stride de 2.\n",
    "* Resto de capas: Siguen la misma línea, pero toman su tamaño del mapa de características de su capa predecesora. También incrementan la frecuencia de filtros de 32 a 64. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08ef662-89b0-47bb-980f-b0860564a6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary() ## Revisión del Modelo creado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d38aaf-a96c-4b40-b462-04dcef54fbcb",
   "metadata": {},
   "source": [
    "A continuación, añadimos las capas densamente conectadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5ec90d-14ec-4cae-9ff9-9b9ce4503687",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dac6c70-fc2d-4a18-878e-99680779369f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c5c4a8a-95b2-4196-baae-a64944d9681e",
   "metadata": {},
   "source": [
    "**Entrenamiento del Modelo:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e72ca2-03f4-4f6b-9734-e3b61799b3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_images, train_labels, epochs=10, \n",
    "                    validation_data=(test_images, test_labels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f3a0a095-b502-4077-9299-005a350b683b",
   "metadata": {},
   "source": [
    "**Evaluación del Modelo:**\n",
    "\n",
    "Hemos conseguido un acierto del 71%, pero se podría mejorar con las técnicas que veremos a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1891515-8ec2-47c7-bd61-adf3136eb010",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print(test_acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b7096a5-6e38-4f5c-99c5-549b9aa3e368",
   "metadata": {},
   "source": [
    "### **Trabajano con Datasets pequeños:** ###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c03ca439-a12f-4f62-a216-6103c9fd1bd7",
   "metadata": {},
   "source": [
    "Cuando no disponemos de millones de imágenes is complicado entrenar una CNN desde cero que ofrezca muy buenos resultados. A continuación una seria de técnicas para mejorar los resultados cuando disponemos de Datasets pequeños con miles de imágenes (que no millones).\n",
    "\n",
    "**Aumento de Datos:**\n",
    "\n",
    "Para crear un Dataset más grande a partir de uno más pequeño, evitando el Overfitting, se puede usar una técnica llamada Aumeto de Datos. Funciona simplemente realizando transformaciones al azar en las imágenes de entrada, de manera que nuestro modelo generalice mejor. Estas transformaciones pueden ser compresiones, rotaciones, estiramientos o incluso cambios de color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f302104b-675b-4769-962e-295a0a166777",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905ffdca-b215-470e-b997-fd80e027cece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un Generador de Objetos que transforma imágenes\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range = 40,      # Rango de rotación en grados\n",
    "    width_shift_range = 0.2,  # Rango para cambiar la posición horizontal de la imagen\n",
    "    height_shift_range = 0.2, # Rango para cambiar la posición vertical de la imagen\n",
    "    shear_range = 0.2,        # Rango de la deformación\n",
    "    zoom_range = 0.2,         # Rango de zoom\n",
    "    horizontal_flip = True,   # Rango de volteo horizontal\n",
    "    fill_mode = 'nearest')    # Estrategia de rellenado para puntos fuera de los límites después de las transforamciones\n",
    "\n",
    "# Escogemos una imagen de prueba a transformar\n",
    "test_img = train_images[14]\n",
    "img = img_to_array(test_img)        # Convertimos la imagen a un Array de Numpy\n",
    "img = img.reshape((1,) + img.shape) # Modificamos el tamaño de la imagen\n",
    "\n",
    "# Este bucle es infinito (hasta romperlo), guardamos imágenes en el directorio actual\n",
    "i = 0\n",
    "for batch in datagen.flow(img, save_prefix = 'test', save_format = 'jpeg'): \n",
    "    plt.figure(i)\n",
    "    plot = plt.imshow(img_to_array(batch[0]))\n",
    "    i += 1\n",
    "    if i > 4:\n",
    "        break\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc36333e-08fa-4387-871f-fd18397be6b4",
   "metadata": {},
   "source": [
    "**Modelos Preentrenados:**\n",
    "\n",
    "El modelo construido durante el ejercicio, ha tardado varios minutos en ejecutarse y tan solo ha obtenido un porcentaje de éxito del 70%.\n",
    "\n",
    "Otra de las formas de mejorar dicho comportamiento sin incrementar el Dataset es el uso de CNNs preentrenadas como parte de nuestra propia red personalizada. Podemos utilizar una CNN preentrenada (con millones de imágenes) como punto de partida de nuestro modelo. Esto nos permite tener una gran Base Convolucional antes de incluir nuestro clasificador densamente conectado.\n",
    "\n",
    "**Fine Tuning**\n",
    "\n",
    "Al utilizar una CNN preentrenada, habitualmente nos gustará retocar las capas finales de la Base Convolucional para que trabaje mejor con nuestro problema específico. Las primeras capas de CNN preentrenado serán muy buenas extrayendo características de bajo nivel y bordes, cosas que son similares en cualquier imagen. Las capas posteriores son mejores identificando características muy específicas como formas o incluso ojos. Ajustando las capas finales, podemos buscar únicamente aquellas características relevantes para nuestro problema.\n",
    "\n",
    "**[Utilizando un Modelo Preentrenado + Fine Tuning](https://www.tensorflow.org/tutorials/images/transfer_learning)**\n",
    "\n",
    "**Módulos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61598416-76e4-4920-be13-9162c3858976",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b303d532-6d41-4ba4-9f9c-e3809125e6a3",
   "metadata": {},
   "source": [
    "**Dataset**\n",
    "\n",
    "El conjunto de datos que vamos a utilizar en este ejemplo, contiene varios miles de imágenes de gatos y perros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59b7229-ad10-4a61-900b-69767e80bd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = r'C:\\Users\\Jairo\\Documents\\Trabajo\\Formacion\\IA\\Python\\Ejemplos\\Datasets\\PetImages'\n",
    "CATEGORIES = ['Dog', 'Cat']\n",
    "IMG_SIZE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e885ac97-8bb4-4d61-bc40-5367e6900df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sirve para ver un ejemplo del resultado obtenido\n",
    "for category in CATEGORIES:\n",
    "    path = os.path.join(DATADIR, category) # Path en el que se encuentran las imágenes de cada categoría (NO PUEDE CONTENER ACENTOS)\n",
    "    for img in os.listdir(path):\n",
    "        img_array = cv2.imread(os.path.join(path, img),cv2.IMREAD_GRAYSCALE)\n",
    "        plt.imshow(img_array, cmap='gray')\n",
    "        plt.show()\n",
    "        break\n",
    "    break\n",
    "\n",
    "print(img_array)\n",
    "\n",
    "new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "plt.imshow(new_array, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a093591a-f029-4750-992d-710d86a360d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparamos el Dataset para pasarle al modelo\n",
    "training_data = [] # Inicializamos el vector que contendrá los datos de entrenamiento\n",
    "def create_training_data():\n",
    "    for category in CATEGORIES:\n",
    "        path = os.path.join(DATADIR, category) # Path en el que se encuentran las imágenes de cada categoría (NO PUEDE CONTENER ACENTOS)\n",
    "        class_num = CATEGORIES.index(category) # Identificamos los valores \"Dog\"y \"Cat\" con 0 y 1 para trabajar con valores numéricos\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
    "                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
    "                training_data.append([new_array, class_num])\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "create_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27670829-f8ee-49fe-a744-68cd72b2ca8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(training_data))\n",
    "print(training_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112c905d-7b27-4602-b16f-7552f182efd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mezclamos los datos para que el modelo trabaje mejor en el proceso de entrenameinto\n",
    "random.shuffle(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8072da-137b-4520-bd98-3b8fb476addb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partimos el dataset en los vectores de Características y Etiquetas\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for features, label in training_data:\n",
    "    x.append(features)\n",
    "    y.append(label)\n",
    "\n",
    "# Convertimos los array en un array de numpy para poder pasárselos al modelo\n",
    "# El primer parámetro del reshape recoge el número de características (-1 indica cualquier valor)\n",
    "x = np.array(x).reshape(-1, IMG_SIZE, IMG_SIZE, 1) # Si quieremos hacer el entrenamiento con las imágenes en color, hay que modificar el último parámetro de 1 a 3 (R, G, B)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d643ff17-3308-4449-99a1-2775c66764d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x[0])\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a64baa-6fad-4841-b083-bde48c527ec0",
   "metadata": {},
   "source": [
    "En ocasiones querremos guardar la estructura de objetos de Python de manera persistente en archivos y luego recuperarlos. Por ejemplo, para guardar y cargar modelos de aprendizaje automático, almacenar configuraciones de aplicaciones, o cualquier caso en el que se necesite preservar la estructura y el contenido de objetos complejos. La siguiente celda hace uso del módulo pickle para guardar los datos resultantes (no es necesario ejecutar esta parte).\n",
    "\n",
    "* *open*: Abrir un archivo para lectura o escritura de objetos serializados. Los objetos serializados son objetos de Python que han sido convertidos en una secuencia de bytes. Parámetros:\n",
    "    * file: Nombre del archivo a abrir (si no existe, lo crea).\n",
    "    * mode: Modo de apertura del archivo. Los valores posibles son \"r\" para lectura, \"w\" para escritura, \"rb\" para lectura binaria y \"wb\" para escritura binaria.\n",
    "* *dump*: Serializar un objeto de Python en una secuencia de bytes. La serialización es el proceso de convertir un objeto en una forma que pueda ser almacenada o transmitida. Parámetros:\n",
    "    * obj: Objeto a serializar.\n",
    "    * file: Archivo u objeto de flujo en el que se escribirá el objeto serializado.\n",
    "* *close*: Cerrar un flujo de datos que se está utilizando para serializar o deserializar objetos. Parámetros:\n",
    "    * file: Archivo u objeto de flujo a cerrar.\n",
    "* *load*: Deserializar un objeto de Python a partir de una secuencia de bytes. La deserialización es el proceso de convertir una secuencia de bytes en un objeto de Python. Parámetros:\n",
    "    * file: Archivo u objeto de flujo del que se leerá el objeto deserializado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265ec50d-35cb-4f63-958b-0d6f233ed91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Guardamos los vectores de Características y Etiquetas creados\n",
    "pickle_out = open('x.picle', 'wb')\n",
    "pickle.dump(x, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open('y.picle', 'wb')\n",
    "pickle.dump(y, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "# Podríamos recuperarlos de este modo de ser necesario\n",
    "pickle_in = open('x.picle', 'rb')\n",
    "x = pickle.load(pickle_in)\n",
    "pickle_in.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5d4e4b-d26b-4219-af17-426a3851423e",
   "metadata": {},
   "source": [
    "**Preprocesamiento de Datos:**\n",
    "\n",
    "El último paso antes de crear nuestro modelo, es preprocesar los datos. Es necesario aplicar algunas transformaciones a nuestros datos antes de pasárselos al modelo. En este caso, vamos a combertir el color de cada pixel de un valor entre 0 y 255 a un valor entre 0 y 1 (dividiendo cada valor por 255). Valores más pequeños facilitarán al modelo el tratamiento de los mismos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7535489d-80b8-4643-a3a0-9185f5811684",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3898783-ae9c-4b6c-9f0f-adc673c3d55e",
   "metadata": {},
   "source": [
    "**Arquitectura de la Red Neuronal Convolucional:**\n",
    "\n",
    "Una arquitectura común para las CNNs es una pila de capas Conv2D y MaxPooling2D, seguidas por unas pocas capas densamente conectadas. La idea es que las capas convolucionales y de maxPooling (su objetivo es simplificar los Mapas de Características resultantes) extraen las características importantes de la imagen (Filtros). Tras lo cual las capas densamente conectadas determinan la clase a la que pertenece la imagen. Comenzamos con la construcción de la Base Convolucional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "137ce5b0-406c-4b99-9ea3-7f150d8d6396",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', input_shape=x.shape[1:])) # shape[:1] indica que tome el valor del shape de x\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138cbd3f-fb9d-4ff1-8568-d27f907e4c0d",
   "metadata": {},
   "source": [
    "Como entrada, una CNN toma tensores de forma (altura_imagen, ancho_imagen, canales_color), ignorando el tamaño del lote.\n",
    "\n",
    "Hemos creado 3 Capas Convolucionales y 2 Capas de MaxPooling entre ellas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e70e22-753b-4de1-852e-b31948c3affe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary() ## Revisión del Modelo creado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53562329-6b34-4822-af38-f58835753e91",
   "metadata": {},
   "source": [
    "A continuación, añadimos las capas densamente conectadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33bd60b8-b55d-421e-a50d-c1239533679c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65cd4bf-e18f-4294-870f-5f1bf55ad565",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f8c25e5-3808-42a2-9d03-8542ea9857e5",
   "metadata": {},
   "source": [
    "**Entrenamiento del Modelo:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a9f85ee-04b7-4ba1-ba9d-81d9381f36fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624/624 [==============================] - 234s 371ms/step - loss: 0.0000e+00 - accuracy: 0.4983 - val_loss: 0.0000e+00 - val_accuracy: 0.5060\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x, y, batch_size=32, epochs=1, validation_split=0.1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9ad181c-5378-4701-97aa-ede4b6ecabcb",
   "metadata": {},
   "source": [
    "##### **Evaluación del Modelo:**\n",
    "\n",
    "Hemos conseguido un acierto del 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c0fc140-4000-45f9-a71b-e892f909d653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0xcc44f0fa60>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "#test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "#print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4532b51b-72f5-4258-b825-1b47663b3b50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3716a50f-4084-45de-8d99-cba466706736",
   "metadata": {},
   "source": [
    "# La URL del Dataset 'cats_vs_dogs' no funciona correctamente. Sin ese problema, se podría ejecutar el código que se muestra a continuación en lugar del que hemos visto previamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43797581-cc85-4a42-acb2-db53e0bad16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Es necesario tener instalado el paquete \"ipywidgets\"\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c927e58-b8c1-4380-a850-461f8736c051",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Cargamos y partimos (datos de entrenamiento y datos de evaluación) el Dataset\n",
    "(raw_train, raw_validation, raw_test), metadata = tfds.load(\n",
    "    'cats_vs_dogs',\n",
    "    split=['train[80%]', 'train[80%:90%]', 'train[90%]'],\n",
    "    with_info = True,\n",
    "    as_supervised = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3677e71-088f-49f0-9ea7-2c72a743f93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un objeto función que podemos utilizar para conseguir etiquetas\n",
    "get_label_name = metadata.features['label'].int2str\n",
    "\n",
    "# Mostramos dos imágenes de nuestro Dataset\n",
    "for image, label in raw_train.take(2): # Número de imágenes a mostrar\n",
    "    plt.figure()\n",
    "    plt.imshow(image)\n",
    "    plt.tittle(get_label_name(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0343fc6d-1f7f-4ea1-a6ba-4a4539751021",
   "metadata": {},
   "source": [
    "**Preprocesamiento de los Datos de Entrada**\n",
    "\n",
    "Podemos ver como el tamaño de cada imagen es distinto. Nosotros necesitamos que el tamaño sea equivalente y para ello creamos la siguiente función:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258700d1-1031-4750-9f04-bf8081ec6feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 160 ## Todas las imágenes serán de 160x160\n",
    "\n",
    "def format_example (image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = (image/127.5) - 1\n",
    "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    return image, label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
