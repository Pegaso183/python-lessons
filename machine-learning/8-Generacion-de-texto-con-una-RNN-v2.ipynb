{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkrrCnFMPbFd"
   },
   "source": [
    "### **Generación de Texto con una RNN** ###\n",
    "\n",
    "Vamos a utilizar un RNN para [generar texto](https://www.tensorflow.org/text/tutorials/text_generation). Le mostraremos a la red una muestra de lo que queremos y aprenderá cómo escribir una versión por si misma. Para hacerlo, vamos a utilizar un modelo de predicción de caracteres que tomará como entrada una secuencia de longitud variable y predice el siguiente caráter. Si lo utilizamos de manera recurrente, conseguiremos que se cree un texto.\n",
    "\n",
    "**Modulos:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B4lGb1CwODKh"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w2jzJJuhQyrL"
   },
   "source": [
    "**Dataset:**\n",
    "\n",
    "Vamos a utilizar como Dataset de entrada para entrenar nuestro modelo parte de la obra Romeo y Julieta de Shakespeare.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V9ifwBXMR2aU",
    "outputId": "5fa315d8-fc5f-4cee-e19e-ed33f15ece17"
   },
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQRXkEStSmO4"
   },
   "source": [
    "Si quisiéramos, podríamos utilizar un texto propio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z7L1OLiaStHT"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "path_to_file = list(files.upload().keys())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTdJpAEdTIGD"
   },
   "source": [
    "**Leer contenido del documento de entrada:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IIhhXS30TO55",
    "outputId": "0a058394-23fa-40fc-8744-d1a7fb0d298a"
   },
   "outputs": [],
   "source": [
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "print('Longitud del texto: {} caracteres'.format(len(text)))\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yuoislveURh3"
   },
   "source": [
    "**Preprocesar la entrada de datos:**\n",
    "\n",
    "La entrada de datos está en formato texto. Sin embargo, nuestro modelo necitará una entrada de datos en formato numérico. Por tanto, debemos codificar nuestro texto antes de pasarlo como entrada al modelo. Vamos a codificar cada carácter como un entero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z7Dt5wc8Tvib"
   },
   "outputs": [],
   "source": [
    "vocab = sorted(set(text))                     # Ordenamos cada carácter único existente en el texto e entrada\n",
    "idx2char = np.array(vocab)                    # Convertimos la lista en un array para poder acceder a cada carácter por su índice\n",
    "char2idx = {u:i for i, u in enumerate(vocab)} # Nos permitirá obtener el índice de cada carácter\n",
    "\n",
    "# La siguiente función devuelve un array NumPy creado a partir del texto de entrada\n",
    "def text_to_int(text):\n",
    "  return np.array([char2idx[c] for c in text]) # Expresión de list comprehension que ejecuta un bucle For...\n",
    "  # int_array = []\n",
    "  # for c in text:\n",
    "  #   int_array.append(char2idx[c])\n",
    "  # return np.array(int_array)\n",
    "\n",
    "text_as_int = text_to_int(text)\n",
    "\n",
    "# La siguiente función devuelve un Texto creado a partir de un Array NumPy (permite revertir la codificación)\n",
    "def int_to_text(ints):\n",
    "  try:\n",
    "    ints = int.numpy()\n",
    "  except:\n",
    "    pass\n",
    "  return ''.join(idx2char[ints])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yf5uzAzkTyqC"
   },
   "outputs": [],
   "source": [
    "print(vocab)\n",
    "print(list(enumerate(vocab)))\n",
    "print(char2idx)\n",
    "print('Texto:', text[:20])\n",
    "print('Codificado:', text_as_int[:20])\n",
    "print(int_to_text(text_to_int(text[:20])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BzLN8GoHoo6w"
   },
   "source": [
    "**Creación de ejemplos:**\n",
    "\n",
    "El objetivo es pasarle al modelo una secuencia y que nos devuelva el siguiente carácter. Por tanto, necesitamos partir el texto en muchos secuencias cortas para entrenar nuestro modelo.\n",
    "\n",
    "Los ejemplos de entrenamiento que vamos a crear toman secuencias de *seq_length* como entrada y devuelve secuencias de *seq_length* como salida (secuencia de entrada movida un carácter a la derecha - entrada: Hell | salida: ello-)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BNsgaPnCVkUs"
   },
   "outputs": [],
   "source": [
    "seq_lenght = 100                                # Longitud de las secuencas para entrenamiento del modelo\n",
    "examples_per_eposch = len(text)//(seq_lenght+1) # Calculamos los ejemplos que obtenemos del texto inicial (+1 en el denominador para recoger el texto restante)\n",
    "\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)     # Cargamos todo el texto de entrada en un Tensor (contiene valores enteros)\n",
    "sequences = char_dataset.batch(seq_lenght+1, drop_remainder=True) # Cada secuencia será de seq_lenght + 1 para contener tanto la entrada de datos como su etiqueta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfeiDGOAwwdn"
   },
   "source": [
    "Una vez disponemos de las secuencas, debemos partirlas en entradas y salidas (etiquetas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G5gb-cKurJ5T"
   },
   "outputs": [],
   "source": [
    "def split_imput_target(chunk):   # Ejemplo: hello\n",
    "  input_text = chunk[:-1]        # hell\n",
    "  target_text = chunk[1:]        # ello\n",
    "  return input_text, target_text # hell, ello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K9QSlX1AwVCt"
   },
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_imput_target) # Utilizamos el map para aplicar la fucnión a cada elemento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2b6VXi__yGv7"
   },
   "outputs": [],
   "source": [
    "for x, y in dataset.take(2):\n",
    "  print('\\n\\nEXAMPLE\\n')\n",
    "  print('INPUT')\n",
    "  print(int_to_text(x))\n",
    "  print('\\nOUTPUT')\n",
    "  print(int_to_text(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "trDq6-DV0Qem"
   },
   "source": [
    "Por último, debemos preparar los datos de entrada para el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I4hcpqmkzGc3"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64         # Cantidad de ejemplos para training por Batch\n",
    "VOCAB_SIZE = len(vocab) # Cantidad de caracteres únicos existentes en nuestro vocabulario\n",
    "EMBEDDING_DIM = 256\n",
    "RNN_UNITS = 1024\n",
    "\n",
    "BUFFER_SIZE = 10000 # Cantidad de elementos de la secuencia que considera para el Suffle\n",
    "\n",
    "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zd9nbHLx1UP4"
   },
   "source": [
    "**Construcción del Modelo:**\n",
    "\n",
    "Vamos a utilizar una capa de Embedding, una capa LSTM y una capa Densa con un nodo por cada carácter existente en nuestro diccionario. La capa densa nos devolverá la distribución de probabilidades sobre toos sus nodos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mQB7q5ag2a-R"
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Embedding(vocab_size, embedding_dim,             # Capa de Embedding\n",
    "                                batch_input_shape=[batch_size, None]), # El None indica que no sabemos el tamaño de cada elemento dentro del Batch\n",
    "      tf.keras.layers.LSTM(rnn_units,\n",
    "                           return_sequences=True,\n",
    "                           stateful=True,\n",
    "                           recurrent_initializer='glorot_uniform'),\n",
    "      tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model\n",
    "\n",
    "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Gf0aTOM4xsG"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vgBQjHk56Zfu"
   },
   "source": [
    "**Función de Pérdida:**\n",
    "\n",
    "Es neceario definir la funión de pérdia para evaluar el modelo. Esto lo hacemos porque el modelo devolverá como salida un Tensor de (64, 100, 65) (Elementos en el Batch, Tamaño de cada Elemento, Nodos de salida con la distribución de probabilidades). Antes vamos a analizar la salida que se espera del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XxzWFixr4_O_"
   },
   "outputs": [],
   "source": [
    "# Ejemplo de salida del modelo (se lo estamos pidiendo al modelo sin entrenamiento)\n",
    "for input_example_batch, target_example_batch in data.take(1):\n",
    "  example_batch_predictions = model(input_example_batch) # Le pedimos al modelo una predicción\n",
    "  print(example_batch_predictions.shape, '# (batch_size, sequence_lenght, vocab_size)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ro21ay-s80mD",
    "outputId": "a3b3fccd-6349-4327-bd89-6aff1d0c2f4b"
   },
   "outputs": [],
   "source": [
    "print(len(example_batch_predictions))  # Array de 64 arrays (elementos dentro del Batch)\n",
    "pred = example_batch_predictions[0]    # Le hemos indicado al modelo que la capa LSTM tenga 'return_sequences=True', por tanto, nos devolverá una distribución de probabiliades por cada Paso de Tiempo (Ejemplo 'Hello': T1 H, T2 He, ...)\n",
    "print(len(pred))\n",
    "print(example_batch_predictions[0][0]) # Distribución de probabilidades del primer paso de Tiempo (65 valores numéricos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjP8LFvrHA6M"
   },
   "source": [
    "Si queremos determinar el carácter predicho, tenemos que elegir un valor basado en la distribución de probabilidades. Esto lo haremos con la función tf.random.categorical.\n",
    "\n",
    "Dicha función toma las distribuciones de probabilidad que devuelve el modelo y realiza muestreos categóricos, devolviendo los índices de las muestras seleccionadas. Por lo tanto, sampled_indices contendrá los índices de las palabras seleccionadas aleatoriamente según las distribuciones de probabilidad proporcionadas por pred. Estos índices representarán las palabras predichas en la siguiente secuencia generada por el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GMstePBU9Fqi"
   },
   "outputs": [],
   "source": [
    "sampled_indices1 = tf.random.categorical(pred, num_samples=1) # Devolvemos una sola muestra para cada distribución de probabilidades (100), que será el carácter predicho\n",
    "\n",
    "sampled_indices = np.reshape(sampled_indices1, (1, -1))[0]   # Pasamos las predicciones resultantes de cada Paso de Tiempo a un Tensor de Shape (100,0) para pasarlas a la función int_to_char()\n",
    "predicted_chars = int_to_text(sampled_indices)               # Predicción del siguiente carácter en cada Paso de Tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nNKjl0BSF0Vm"
   },
   "outputs": [],
   "source": [
    "print(sampled_indices1.shape) # Vector de 100 elementos con 1 elemento cada uno\n",
    "print(sampled_indices.shape)  # Vector de 100 elementos\n",
    "print(sampled_indices1)\n",
    "print(sampled_indices)\n",
    "print(predicted_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47ocK5LVK794"
   },
   "source": [
    "Vamos a crear una función de Pérdida que nos permita comparar esa salida con la salida esperada y nos devuelva una representación numérica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "c9CRsGI4Jq7w"
   },
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3ujTfDmMTfh"
   },
   "source": [
    "**Compilando el Modelo:**\n",
    "\n",
    "En este momento, podemos pensar en nuestro problema como un problema de clasificación, donde el modelo predice la probabilidad de que cada uno de los posibles carácteres del vocabulario sea el próximo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "wDqzGlBtMx0H"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TqOSDkjFM-bs"
   },
   "source": [
    "**Creación de Checkpoints:**\n",
    "\n",
    "Los Checkpoints son puntos de control en el entrenamiento del modelo en los que se guarda una copia del modelo y su estado en ese momento particular.\n",
    "\n",
    "Vamos a crear y configurar nuestros Checkpoints. Esto nos permitirá cargar nuestro modelo desde un Checkpoint y continuar con el proceso de entrenamiento a partir de ese punto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "yMh5uep-NDKT"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './Training_Checkpoints'                        # Creamos el directorio en el que guardaremos los Checkpoings\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}') # Nombre de los ficheros de Checkpoint\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_wights_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aykp99VCPeLl"
   },
   "source": [
    "**Entrenamiento del Modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-I5vsys4PoF3",
    "outputId": "63ac2c1b-d268-4860-826b-2f9e8a598a6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "172/172 [==============================] - ETA: 0s - loss: 2.5873 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./Training_Checkpoints\\ckpt_1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./Training_Checkpoints\\ckpt_1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172/172 [==============================] - 1871s 11s/step - loss: 2.5873\n",
      "Epoch 2/40\n",
      " 52/172 [========>.....................] - ETA: 21:01 - loss: 2.0051"
     ]
    }
   ],
   "source": [
    "history = model.fit(data, epochs=40, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-okACi4vP3Mj"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
