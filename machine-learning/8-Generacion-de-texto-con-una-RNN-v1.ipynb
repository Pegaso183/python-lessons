{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Generación de Texto con una RNN** ###\n",
        "\n",
        "Vamos a utilizar una RNN basada en caracteres para [generar texto](https://www.tensorflow.org/text/tutorials/text_generation). Le mostraremos a la red una muestra de lo que queremos y aprenderá cómo escribir una versión por si misma. Para hacerlo, vamos a utilizar un modelo de predicción de caracteres que tomará como entrada una secuencia de longitud variable y predice el siguiente caráter. Si lo utilizamos de manera recurrente, conseguiremos que se cree un texto.\n",
        "\n",
        "**Modulos:**"
      ],
      "metadata": {
        "id": "O7R1NgxFr7CF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-uJK1-uJrj3g"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset:**\n",
        "\n",
        "Vamos a utilizar como Dataset de entrada para entrenar nuestro modelo parte de la obra Romeo y Julieta de Shakespeare."
      ],
      "metadata": {
        "id": "qezFV8lStKXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "id": "HL2u6P9ztSND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lectura de Datos:**"
      ],
      "metadata": {
        "id": "xmOtZ512ttrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos los datos de entrada a partir del fichero\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "print(f'Logitud del texto: {len(text)} caracteres')\n",
        "print(text[:250])"
      ],
      "metadata": {
        "id": "RWLuhEF0tyVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(text)) # Ordenamos cada carácter único existente en el texto e entrada\n",
        "print(f'{len(vocab)} caracteres únicos')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nF6vkephuqde",
        "outputId": "9ebd7efc-8161-49fe-8768-2f7b43861a48"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 caracteres únicos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocesar la entrada de datos:**\n",
        "\n",
        "La entrada de datos está en formato texto. Antes del entrenamiento, hay que convertir las cadenas en una representación numérica.\n",
        "\n",
        "La capa tf.keras.layers.StringLookup puede convertir cada carácter en un ID numérico. Solo es necesario que el texto se divida en tokens primero. Para ello podemos utilizar la función tf.strings.unicode_split... Ejemplo..."
      ],
      "metadata": {
        "id": "l2xshHGIuppr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars # La b por delante de cada carácter representa que son cadenas de bytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmLJc5T7u2l6",
        "outputId": "111008fb-6bfb-4e0e-8ce9-16fbbde0843e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos la capa [tf.keras.layers.StringLookup](https://www.tensorflow.org/api_docs/python/tf/keras/layers/StringLookup)"
      ],
      "metadata": {
        "id": "kzSWOp8bxn4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)"
      ],
      "metadata": {
        "id": "byAs3kGSx6tm"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roTTjuuoyqrS",
        "outputId": "32466b7a-7289-4032-f723-ac3db95b799b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dado que nuestro objetivo es generar texto, es necesario invertir esta representación y recuperar cadenas legibles por humanos a partir de ella. Para esto, se puede usar [tf.keras.layers.StringLookup](https://www.tensorflow.org/api_docs/python/tf/keras/layers/StringLookup)(..., invert=True).\n",
        "\n",
        "Aquí, en lugar de pasar el vocabulario original generado con sorted(set(text)), usamos el método get_vocabulary() de la capa tf.keras.layers.StringLookup para que los tokens [UNK] se configuren de la misma manera."
      ],
      "metadata": {
        "id": "jUdnTX1fzUJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "d-L5JKDnzoUB"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esta capa recupera los caracteres de los vectores de ID y los devuelve como un tf.RaggedTensor de caracteres."
      ],
      "metadata": {
        "id": "gIOVkoF-z6bt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ByVqvnYz-k2",
        "outputId": "36c15223-2293-4b18-b47b-04b3ed2bdb5a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos usar tf.strings.reduce_join para volver a unir los caracteres en cadenas."
      ],
      "metadata": {
        "id": "xevzZika0Nnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGdGau2u0R5w",
        "outputId": "5a1f3d8b-1546-4070-b023-3baefcc3c933"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "MdpoXe5k0Uda"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**La tarea de predicción:**\n",
        "\n",
        "Dado un carácter, o una secuencia de caracteres, ¿cuál es el próximo carácter más probable? Ésta es la tarea para la que estamos entrenando al modelo. La entrada al modelo será una secuencia de caracteres, y entrenamos el modelo para predecir la salida: el siguiente carácter en cada paso de tiempo.\n",
        "\n",
        "Dado que las RNN mantienen un estado interno que depende de los elementos vistos anteriormente, dados todos los caracteres computados hasta este momento, ¿cuál es el siguiente carácter?\n",
        "\n",
        "**Crear ejemplos y objetivos de entrenamiento:**\n",
        "\n",
        "Dividimos el texto en secuencias de ejemplo. Cada secuencia de entrada contendrá caracteres *seq_length* del texto.\n",
        "\n",
        "Para cada secuencia de entrada, los objetivos correspondientes contienen la misma longitud de texto, excepto que se desplaza un carácter a la derecha.\n",
        "\n",
        "Dividimos el texto en partes de *seq_length*+1 . Por ejemplo, digamos que seq_length es 4 y nuestro texto es \"Hola\". La secuencia de entrada sería \"Hell\" y la secuencia de destino \"ello\".\n",
        "\n",
        "Para hacer esto, primero usamos la función tf.data.Dataset.from_tensor_slices para convertir el vector de texto en una secuencia de índices de caracteres."
      ],
      "metadata": {
        "id": "lpGxNNmh1ts6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XHZ3a4N2Jtq",
        "outputId": "e08fa303-9408-4f93-e624-4a7936c9875e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "\n",
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "id": "bM8pk6Vc3QZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)"
      ],
      "metadata": {
        "id": "kbmsUxNW3hvF"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El método batch nos permite convertir fácilmente estos caracteres individuales en secuencias del tamaño deseado."
      ],
      "metadata": {
        "id": "DsUWAcvv3muU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y79_YKUX3sUh",
        "outputId": "e2195fb2-a96e-4bb1-e3d9-29f68be50a6f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos volver a unir los tokens en cadenas para que sea más sencillo visualizar lo que estamos haciendo."
      ],
      "metadata": {
        "id": "zA7M1QyR396d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fADpefGK331c",
        "outputId": "78c4e027-734e-418e-d74d-d96a84572d89"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para el entrenamiento, necesitamos un conjunto de datos de (input, label). Donde input y label son secuencias. En cada paso de tiempo, la entrada es el carácter actual y la etiqueta es el siguiente carácter.\n",
        "\n",
        "Creamos una función que toma una secuencia como entrada, la duplica y la cambia para alinear la entrada y la etiqueta para cada paso de tiempo:"
      ],
      "metadata": {
        "id": "InJ4wuxH4Htq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):   # Ejemplo: hello\n",
        "    input_text = sequence[:-1]      # hell\n",
        "    target_text = sequence[1:]      # ello\n",
        "    return input_text, target_text  # hell, ello"
      ],
      "metadata": {
        "id": "Huc6WU4T4Tjd"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2Lfsefw40Qr",
        "outputId": "ecea0a78-531a-45d1-d07a-92854a3b67b8"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "RgZA85fk422u"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oxzdvgq_5fs9",
        "outputId": "5c782d1c-eee1-43bc-85a9-fb779b97da46"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Crear lotes de entrenamiento:**\n",
        "\n",
        "Una vez dividido el texto en secuencias manejables, hay que mezclar los datos y empaquetarlos en lotes antes de introducirlos en el modelo."
      ],
      "metadata": {
        "id": "CkboJLR95nwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64     # Cantidad de ejemplos para training por Batch\n",
        "\n",
        "BUFFER_SIZE = 10000 # Cantidad de elementos de la secuencia que considera para el Suffle\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAPfF-8d5-4I",
        "outputId": "3aee21b1-a77c-4d3e-8ec1-dcb1533aaaac"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Construcción del Modelo:**\n",
        "\n",
        "Definimos el modelo como una subclase keras.Model (para más info [Creación de nuevas capas y modelos a través de subclases](https://www.tensorflow.org/guide/keras/making_new_layers_and_models_via_subclassing) ).\n",
        "\n",
        "Creamos un modelo con 3 capas: La capa de embedding se encarga de la representación de las palabras, la capa GRU captura patrones secuenciales, y la capa densa genera la salida final del modelo.\n",
        "\n",
        "* tf.keras.layers.Embedding: La capa de entrada. Una tabla de búsqueda entrenable que asignará cada ID de carácter a un vector con dimensiones embedding_dim\n",
        "* tf.keras.layers.GRU: un tipo de RNN con units=rnn_units (podríamos haber usado una capa LSTM).\n",
        "* tf.keras.layers.Dense: la capa de salida, con *vocab_size* nodos de salida. Produce un logit (antes de que se aplique la función de activación en la capa de salida, la salida se llama logit) por cada carácter del vocabulario. La capa densa nos devolverá la distribución de probabilidades sobre todos sus nodos."
      ],
      "metadata": {
        "id": "_PEr8uAr6dOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(vocab) # Longitud del vocabulario\n",
        "embedding_dim = 256     # Dimensiones de la capa de Embedding\n",
        "rnn_units = 1024        # Número de unidades RNN\n",
        "\n",
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):                 # Método para inicializar la instancia del modelo\n",
        "    super().__init__(self)                                                  # Llama al método __init__ de la clase base (tf.keras.Model). Esta línea asegura que se realicen todas las inicializaciones necesarias de la clase base antes de agregar las capas personalizadas.\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)   # La capa de embedding convierte números enteros (identificadores de palabras) en vectores densos de embedding_dim dimensiones. Esto es crucial para representar semánticamente las palabras en un espacio continuo.\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,                               # Capa de tipo GRU (Gated Recurrent Unit), una variante de las capas recurrentes en las redes neuronales. La GRU es capaz de capturar patrones secuenciales en los datos\n",
        "                                   return_sequences=True,                   # Nos devolverá una distribución de probabiliades por cada Paso de Tiempo (Ejemplo 'Hello': T1 H, T2 He, ...)\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs                                                              # Secuencia de entrada al modelo\n",
        "    x = self.embedding(x, training=training)                                # La secuencia de entrada se pasa a través de la capa de embedding. Esta capa convierte los identificadores de palabras en vectores densos. El parámetro training se utiliza para indicar si el modelo está en modo de entrenamiento o inferencia.\n",
        "    if states is None:                                                      # Se verifica si se proporciona un estado inicial (states).\n",
        "      states = self.gru.get_initial_state(x)                                # Si no se proporciona, se obtiene un estado inicial\n",
        "    x, states = self.gru(x, initial_state=states, training=training)        # La capa GRU procesa la secuencia y devuelve tanto la secuencia resultante (x) como el nuevo estado de la celda (states).\n",
        "    x = self.dense(x, training=training)                                    # Esta capa produce la salida final del modelo\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "ntaOBndf65Ls"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "K-MbvMbK8CWt"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Probamos el comportamiento del Modelo:**"
      ],
      "metadata": {
        "id": "7ONRCUDRFaoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxgNF6rhFeWT",
        "outputId": "62a7419b-5426-4d8e-d6de-77581942f634"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para obtener predicciones reales del modelo, hay que muestrear la distribución de salida para obtener índices de caracteres reales. Esta distribución está definida por los logits sobre el vocabulario de caracteres.\n",
        "\n",
        "Es importante tomar muestras de esta distribución, ya que tomar el argmax de la distribución puede hacer que el modelo se atasque fácilmente en un bucle.\n",
        "\n",
        "Probamos el primer ejemplo en el lote. Esto nos da, en cada paso de tiempo, una predicción del siguiente índice de caracteres."
      ],
      "metadata": {
        "id": "zZxkqY75Fz_W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
        "sampled_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTbDgbHfF9p3",
        "outputId": "22a239ce-b3de-4be8-b46f-8e275dc34815"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 7, 31, 40, 39, 53, 49, 23,  5,  7, 55, 16,  0, 32,  2, 49, 14, 45,\n",
              "       51, 35, 63, 40, 39, 16, 36, 57, 59, 28, 42, 44,  8, 24, 26, 58, 54,\n",
              "       29,  3, 53, 28, 22, 35, 17, 56, 17, 32, 42, 39, 60, 40, 62, 31, 54,\n",
              "       43, 37, 63, 10,  5,  6, 61, 36, 31, 12, 10, 57, 52, 24, 60,  0, 10,\n",
              "       37, 42, 13, 54, 54, 31,  2, 51,  1, 36, 44, 25, 38, 29, 53, 55, 54,\n",
              "       17, 17,  1, 18, 62, 19, 11, 56, 55, 26, 42, 24, 21, 46, 26])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos decodificarlo para ver el texto predicho por este modelo no entrenado."
      ],
      "metadata": {
        "id": "RsH5oSEhGcHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "id": "XEFU3NdYGg6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Entrenamiento del Modelo**\n",
        "\n",
        "En este punto, el problema se puede tratar como un problema de clasificación estándar. Dado el estado anterior de RNN y la entrada de este paso de tiempo, prediga la clase del siguiente carácter.\n",
        "\n",
        "**Añadimos un optimizador y una función de pérdida:**\n",
        "\n",
        "La función de pérdida estándar tf.keras.losses.sparse_categorical_crossentropy funciona en este caso porque se aplica en la última dimensión de las predicciones.\n",
        "\n",
        "Debido a que el modelo devuelve logits, hay que configurar el indicador from_logits."
      ],
      "metadata": {
        "id": "cPpH_cyXGrJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "0T6g2rDzHDZi"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnAavN_4HGNr",
        "outputId": "a76c319e-d240-47c0-bb0f-a5c04d5df694"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.189786, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un modelo recién inicializado no debería estar demasiado seguro de sí mismo, todos los logits de salida deberían tener magnitudes similares. Para confirmar esto podemos comprobar que la exponencial de la pérdida media es aproximadamente igual al tamaño del vocabulario. Una pérdida mucho mayor significa que el modelo está seguro de sus respuestas incorrectas y está mal inicializado."
      ],
      "metadata": {
        "id": "WoSmut5SHVST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PziMsjEZHeEl",
        "outputId": "8da85722-db5f-49ef-9c45-b5309439fd72"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66.00866"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configuramos el procedimiento de entrenamiento utilizando el método tf.keras.Model.compile."
      ],
      "metadata": {
        "id": "nGR-Zz7SHmz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "fJBQwJFiHsQq"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Configuración de Puntos de Control (Checkpoints):**\n",
        "\n",
        "Usamos un tf.keras.callbacks.ModelCheckpoint para asegurarnos de que los puntos de control se guarden durante el entrenamiento."
      ],
      "metadata": {
        "id": "b8JCzBH2HyUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir = './training_checkpoints'                        # Directorio en el que se guardan los Checkpoints\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\") # Nombre de los ficherso de Checkpoint\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "v1KEAnqXH9NP"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ejecutamos el Entrenamiento del Modelo:**"
      ],
      "metadata": {
        "id": "kgpOQgCYKmP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20\n",
        "\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tniCErFKraA",
        "outputId": "f9bfc3a1-dc98-4fc8-a212-90ef6ebc345c"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 1.1831\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 14s 62ms/step - loss: 1.1437\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 12s 62ms/step - loss: 1.1029\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 12s 62ms/step - loss: 1.0595\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 13s 63ms/step - loss: 1.0134\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 13s 62ms/step - loss: 0.9648\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 14s 62ms/step - loss: 0.9139\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 13s 61ms/step - loss: 0.8617\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 12s 62ms/step - loss: 0.8101\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 12s 62ms/step - loss: 0.7600\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 13s 61ms/step - loss: 0.7134\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 0.6711\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 0.6324\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 12s 62ms/step - loss: 0.5977\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 0.5684\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 13s 61ms/step - loss: 0.5444\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 13s 62ms/step - loss: 0.5236\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 13s 62ms/step - loss: 0.5084\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 0.4942\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 0.4823\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generar texto:**\n",
        "\n",
        "La forma más sencilla de generar texto con este modelo es ejecutarlo en un bucle y realizar un seguimiento del estado interno del modelo a medida que se ejecuta.\n",
        "\n",
        "Cada vez que se llama al modelo, se pasa un texto y un estado interno. El modelo devuelve una predicción para el siguiente carácter y su nuevo estado. Vuelva a pasar la predicción y el estado para continuar generando texto.\n",
        "\n",
        "El siguiente código hace una predicción de un solo paso."
      ],
      "metadata": {
        "id": "Zq59d1KtLDtR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None] # Crea una máscara para prevenir que se generen \"[UNK]\"\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "cpGlV3WYLL6V"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "Ligg5VetM_Gh"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si lo ejecutamos en bucle, podemos generar algo de texto. Mirando el texto generado, vemos que el modelo sabe cuándo usar mayúsculas, hacer párrafos e imita un vocabulario de escritura similar al de Shakespeare. Con el pequeño número de épocas de entrenamiento, aún no ha aprendido a formar oraciones coherentes."
      ],
      "metadata": {
        "id": "RLpI33pJNMCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wP_BZIq0NXAe",
        "outputId": "76f3456a-a043-4697-b6f1-6b0e3c9f86d1"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "There wanted colours have been so much the house of Lancaster.\n",
            "Those dwells musicians for authority\n",
            "As well obey agains; against you\n",
            "Medward to help! amidity, is not most?\n",
            "As mistress you mistake.\n",
            "\n",
            "GLOUCESTER:\n",
            "My brother for autidity: the whole heart's doors!\n",
            "Come, follow me, and provide both their faces\n",
            "Of all your people bend me hut.\n",
            "\n",
            "AUTOLYCUS:\n",
            "I am a good Putied and awhile at libe:\n",
            "Embrace may thither shall have lived to speak fort:\n",
            "Help, help! Call of the king's,\n",
            "may song, or wrong too lict but danger\n",
            "Which he profess'd: for I will not be\n",
            "That I shall be well beloved.\n",
            "In the name too well, but lovers'd, thank you\n",
            "Of what is pasting for us unthere.\n",
            "Give me thy tongue.\n",
            "\n",
            "QUEEN MARGARET:\n",
            "Nay, go not from us thus.\n",
            "If he were falsehood he rudesty,\n",
            "To make his valour live it was revolt,\n",
            "Your children's deeds, to do their abuses;\n",
            "And that the sky, if Romeo's nose\n",
            "That torn'd eye or are old, Clubble Edward's moint:\n",
            "Thou yet to-day upon the bowels of the\n",
            "see--O, Outhously undrunged motion\n",
            " \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.128744125366211\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Posibilidades de Mejora:**\n",
        "\n",
        "* Entrenar el modelo durante más épocas.\n",
        "* Modificar la cadena inicial de datos.\n",
        "* Experimentar con una capa de inicio diferente. Se puede intente agregar otra capa RNN para mejorar la precisión del modelo o ajustar el parámetro de temperatura para generar predicciones más o menos aleatorias.\n",
        "\n",
        "Si desea que el modelo genere texto más rápido , lo más fácil que puede hacer es generar el texto por lotes. En el siguiente ejemplo, el modelo genera 5 salidas aproximadamente en el mismo tiempo que se tardó en generar 1 arriba."
      ],
      "metadata": {
        "id": "AZnyRwFMOFvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6zkxOXpO0oE",
        "outputId": "a08b3852-b5a8-4a3d-9516-b9c06606be38"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nThe devil care from Edward's children keep him in the dire,\\nAnd left thee back our highor to the wisest, or our\\nMost buried in place. But leave to fly,\\nThat Tybalt bend like us, in post,\\nSo stands me to the block of state, where nothing\\nflames here we weirs our state; feign Albar-m,\\nWhere stamed his liberty makes, made place pill's lives\\nWith lovers' swords we ento a bower\\nThan you shall buzzer, wronging to your charge,\\nScalibial hablet read of bitter when which time your bitter\\nHe plainers wish the babis.\\n\\nHASTINGS:\\nSo thrive me, death, what was he that utter'd flin?\\nReare the valour of my cheeks.\\nWhat was my weddod took his life?\\nNow prisoners lie thou the rest.\\n\\nDUKE VINCENTIO:\\nMy heart cannot he doubt; for you may know\\nHe let us here another affection.\\nThis is my orato, and thy face to land a treaty\\nAnd do it in the world; and we perching in\\nWernabsit, we'll deny thy crown'd\\nTo know your father's house: his son is daughter,\\nThat when the sweet wants of breath\\nIthair with a black d\"\n",
            " b'ROMEO:\\nThe dwellight wax of lord.\\nThen, taking of the right, which we in love\\nThe lute to me as or foul-flay--a passing queen\\nLore standers proudly dead;\\nTherefore they do not be; tell Warwick from mine eyes.\\nMy virtue let not be sent for.\\n\\nJULIET:\\nI would they well and slew the fault: think him to right.\\n3 KING HENRY VI\\n\\nRIVERS:\\nMadam, his horsehs in the battle rame?\\n\\nROMEO:\\nBut heread the other turn and will: the good old\\nwoman it is zeloked, he small have no need to begin.\\n\\nRoTEO:\\nGood fellows, he brief wanted the command.\\nBut, if ever I should seem of all I please.\\n\\nLUCENTIO:\\nFor this one favour, sir.\\n\\nESCALUS:\\nI thought she bid me: start all the day.\\n\\nGLOUCESTER:\\nCousin one: I know her mother: who knows he is\\nMy stage and recreation.\\n\\nShepherd:\\nWell, he do is, ere I could temper with the sword\\nTo save the ground I cannot,--or else such printest gown,\\nMore lie-still belling with a hundred power,\\nBy mingling tender heart: away for them,\\nBut a hand to have some conference to the prince\\nEve'\n",
            " b\"ROMEO:\\nHold thy petercians hath set to-night.\\n\\nBRUTUS:\\nIt was armorang goos?\\n\\nBIONDELLO:\\nAncelsed by me; go, pet you hencefult content.\\n\\nPETRUCHIO:\\nAlas! foul wrongful body, sparing! madam; therefore help!\\n\\nROMEO:\\nNot possible.\\n\\nMessenger:\\nThe proudest he hath wolk and not proud,\\nBid might have my wife's lost what way't it now.\\n\\nAUTOLYCUS:\\nA little gift of thief! or half moves men; except him with\\nanother--there, with awlieved eight:\\nYet looks fect and offence nor so appeasate;\\nGod point with mine advertissing on a brother's love,\\nTo set the tracting of it. Didst with me.\\n\\nLADY GREY:\\nMy lord, I have a brother is come to know.\\n\\nBUCKINGHAM:\\nWell, then, I can. But charges his death?\\nO, he is with a time quo choose but created\\nEse up those house! are as grave\\nAs ivery man that we have cause\\nOff with the envy and remembrance of the basand: he\\ncannings with abundance that time may wink and low\\nThan you accept us: set down be he's deliver'd.\\n\\nQUEEN MARGARET:\\nPeace, master mayor, these hopes unbroke\"\n",
            " b\"ROMEO:\\nThere is done of this. You are renowned love?\\n\\nJULIET:\\nI must, and let he go set on.\\n\\nFLORIZEL:\\nNo, so enter is,\\nSir chance to standle on your sister.\\n\\nLEONTES:\\nWhat means dis?\\nof this treason: for my daughter would have wouther?\\n\\nBUCKINGHAM:\\nMy life, sir; and tell him in a sea,\\nComes ruch from me here or committed.\\n\\nPage:\\nMy lord, the other did strength and hell:\\nWe can well ask his inclination: follow?\\n\\nANTONIO:\\nAnd better yet but newly considered,\\nYet art thou worthy stabulatuous heart\\nTo hold my own desert:--O, much mislike him err\\nIs' hath foldly suffer: Marcius shall be here,\\nBecause they would all which with a blother's guard\\nOf their pretty ones. Is the dead man\\nthat your estate? I mean to marry her.\\n\\nQUEEN MARGARET:\\nOut! what dost thou on the helm whose danger\\nWhich he me fruitful meal that was my heart!\\nIf any think there was more equallingly\\nThan beat them in her face? now are they would.\\n\\nHORTENSIO:\\nSirrah, the sistee of the chamber.\\n\\nESCALUS:\\nWell, sir, what art thou what\"\n",
            " b\"ROMEO:\\nWhy, he that wishing means the like had\\nHold close the crown on me: but, as I am?\\n\\nGLOUCESTER:\\n'Tis done my daughter gone: but, Offended, use no further.\\n\\nCORIOLANUS:\\nNo, present me once here of that; you have\\nseen the shepherd's nine cit more remembrance?\\nCall Edward flay-want of which he hath,\\nAnd say it was. But to the head is so\\narr breed-shunning in our arms.\\nCousin of Buckingham and thy father's course,\\nWhose father so ill bechanced a purpose,\\nWhich not torced traitor confusion.\\n\\nBRUTUS:\\nHe's a lamb it seldom.\\n\\nClown:\\nSo then believe me, boy.\\n\\nClown:\\nSee, see; what a man you?\\n\\nLord:\\nI'll pay them for secret mother; break our state so;\\nFor it is Warwick king, whose father stomach'd,\\nWhich carrion it ever: coiving he!\\n\\nKING RICHARD II:\\nWe would, madam: therefore enjoy, he longs to seem his battle,\\nThen, to mine eyes, and Greence, set on the case in me\\nFor citizens to be understanding\\nA thousand feasts within my house!\\nPettance our sense us two hours of the stars:\\nYet fear it not n\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.127552032470703\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exportar el Generador de Texto:**\n",
        "\n",
        "Este modelo de un solo paso se puede guardar y restaurar fácilmente, lo que le permite usarlo en cualquier lugar donde se acepte un tf.saved_model."
      ],
      "metadata": {
        "id": "b-qlFMvJPMXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5W4CI8NhPVtw",
        "outputId": "d7f3e60d-30cf-4caf-d86f-a8adc6fd5e40"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7868a4564970>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para utilizar el modelo guardado..."
      ],
      "metadata": {
        "id": "ns41Xyr2Prxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6Vkcyq7PvJT",
        "outputId": "f4689bdd-d760-46f7-86c1-8734f7915e7b"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "The contents of it. Didst yet the king\n",
            "Of all the world to see him: be assured\n",
            "In any perfect to th\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Avanzado: Entrenamiento Personalizado** ###\n",
        "\n",
        "El procedimiento de entrenamiento anterior es simple, pero permite mucho control. Se puede utilizar el maestro forzado que evita que las malas predicciones se retroalimenten al modelo, por lo que el modelo nunca aprende a recuperarse de los errores.\n",
        "\n",
        "Ahora vamos a implementar un ciclo de entrenamiento. Esto brinda un punto de partida si, por ejemplo, desea implementar el aprendizaje del plan de estudios para ayudar a estabilizar la salida de bucle abierto del modelo.\n",
        "\n",
        "La parte más importante de un ciclo de entrenamiento personalizado es la función de paso de entrenamiento.\n",
        "\n",
        "Usamos tf.GradientTape para rastrear los degradados. Para más información sobre este enfoque... [guía de ejecución ansiosa](https://www.tensorflow.org/guide/basics).\n",
        "\n",
        "El procedimiento básico es:\n",
        "\n",
        "1. Ejecutar el modelo y calcular la pérdida bajo un tf.GradientTape.\n",
        "2. Calcular las actualizaciones y aplicarlas al modelo utilizando el optimizador."
      ],
      "metadata": {
        "id": "YNGGjqXvQS1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTraining(MyModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "      inputs, labels = inputs\n",
        "      with tf.GradientTape() as tape:\n",
        "          predictions = self(inputs, training=True)\n",
        "          loss = self.loss(labels, predictions)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      return {'loss': loss}"
      ],
      "metadata": {
        "id": "VN8SfAhUPy07"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La implementación anterior del método train_step sigue las convenciones train_step de Keras. Esto es opcional, pero permite cambiar el comportamiento del paso de tren y seguir usando los métodos Model.compile y Model.fit de keras."
      ],
      "metadata": {
        "id": "AMWunpRwRhde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "DrjAGS2sRmmr"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ],
      "metadata": {
        "id": "tB5Ol41aRvBG"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(dataset, epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t02mkmlpR1qs",
        "outputId": "5a08a904-0820-4101-ab87-a3917631f022"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172/172 [==============================] - 15s 62ms/step - loss: 2.7263\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7868a44bff70>"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si hace falta más control, se puede escribir un ciclo de entrenamiento personalizado completo."
      ],
      "metadata": {
        "id": "aq2XneTpSDNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    mean.reset_states()\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "        logs = model.train_step([inp, target])\n",
        "        mean.update_state(logs['loss'])\n",
        "\n",
        "        if batch_n % 50 == 0:\n",
        "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            print(template)\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print()\n",
        "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "    print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTXGW_IbSJlN",
        "outputId": "24683a61-18d0-4996-aa47-6a56d9c1a41e"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 2.1893\n",
            "Epoch 1 Batch 50 Loss 2.0712\n",
            "Epoch 1 Batch 100 Loss 1.9571\n",
            "Epoch 1 Batch 150 Loss 1.8657\n",
            "\n",
            "Epoch 1 Loss: 1.9935\n",
            "Time taken for 1 epoch 13.55 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 1.7771\n",
            "Epoch 2 Batch 50 Loss 1.7589\n",
            "Epoch 2 Batch 100 Loss 1.7141\n",
            "Epoch 2 Batch 150 Loss 1.6366\n",
            "\n",
            "Epoch 2 Loss: 1.7144\n",
            "Time taken for 1 epoch 12.10 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 1.5961\n",
            "Epoch 3 Batch 50 Loss 1.5859\n",
            "Epoch 3 Batch 100 Loss 1.5854\n",
            "Epoch 3 Batch 150 Loss 1.4921\n",
            "\n",
            "Epoch 3 Loss: 1.5525\n",
            "Time taken for 1 epoch 14.14 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 1.4663\n",
            "Epoch 4 Batch 50 Loss 1.4946\n",
            "Epoch 4 Batch 100 Loss 1.4612\n",
            "Epoch 4 Batch 150 Loss 1.4484\n",
            "\n",
            "Epoch 4 Loss: 1.4531\n",
            "Time taken for 1 epoch 13.20 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 1.3618\n",
            "Epoch 5 Batch 50 Loss 1.4480\n",
            "Epoch 5 Batch 100 Loss 1.4471\n",
            "Epoch 5 Batch 150 Loss 1.3798\n",
            "\n",
            "Epoch 5 Loss: 1.3849\n",
            "Time taken for 1 epoch 12.15 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 1.3302\n",
            "Epoch 6 Batch 50 Loss 1.3669\n",
            "Epoch 6 Batch 100 Loss 1.3334\n",
            "Epoch 6 Batch 150 Loss 1.3154\n",
            "\n",
            "Epoch 6 Loss: 1.3327\n",
            "Time taken for 1 epoch 12.74 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 1.2646\n",
            "Epoch 7 Batch 50 Loss 1.3131\n",
            "Epoch 7 Batch 100 Loss 1.2886\n",
            "Epoch 7 Batch 150 Loss 1.2901\n",
            "\n",
            "Epoch 7 Loss: 1.2886\n",
            "Time taken for 1 epoch 11.67 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 8 Batch 0 Loss 1.2610\n",
            "Epoch 8 Batch 50 Loss 1.2383\n",
            "Epoch 8 Batch 100 Loss 1.2450\n",
            "Epoch 8 Batch 150 Loss 1.2586\n",
            "\n",
            "Epoch 8 Loss: 1.2472\n",
            "Time taken for 1 epoch 11.87 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 9 Batch 0 Loss 1.1552\n",
            "Epoch 9 Batch 50 Loss 1.1884\n",
            "Epoch 9 Batch 100 Loss 1.2262\n",
            "Epoch 9 Batch 150 Loss 1.2378\n",
            "\n",
            "Epoch 9 Loss: 1.2086\n",
            "Time taken for 1 epoch 11.81 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 10 Batch 0 Loss 1.1352\n",
            "Epoch 10 Batch 50 Loss 1.1955\n",
            "Epoch 10 Batch 100 Loss 1.1716\n",
            "Epoch 10 Batch 150 Loss 1.1806\n",
            "\n",
            "Epoch 10 Loss: 1.1683\n",
            "Time taken for 1 epoch 11.95 sec\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    }
  ]
}